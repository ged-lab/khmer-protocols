================================================
1. Quality Trimming and Filtering Your Sequences
================================================

.. shell start

Be aware of your space requirements and obtain an
appropriately sized machine ("instance") and storage ("volume").


On the new machine, run the following commands to update the base
software and reboot the machine::

   apt-get update
   apt-get -y install screen git curl gcc make g++ python-dev unzip default-jre \
              pkg-config libncurses5-dev r-base-core r-cran-gplots python-matplotlib\
              sysstat && shutdown -r now


Install software
----------------

.. clean up previous installs if we're re-running this...

.. ::

   set -x
   set -e
   echo Removing previous installs, if any.
   rm -fr /root/Trimmomatic-*
   rm -f /root/libgtextutils-*.bz2
   rm -f /root/fastx_toolkit-*.bz2
   rm -fr /root/bowtie2*
   rm -fr /root/samtools*

.. ::

   echo Clearing times.out
   mv -f /root/times.out /root/times.out.bak
   echo 1-quality INSTALL `date` >> /root/times.out

Install Bowtie2
::

   cd /root
   curl -L -O 'http://downloads.sourceforge.net/project/bowtie-bio/bowtie2/2.1.0/bowtie2-2.1.0-source.zip?r=http%3A%2F%2Fsourceforge.net%2Fprojects%2Fbowtie-bio%2Ffiles%2Fbowtie2%2F2.1.0%2F&ts=1365392377&use_mirror=superb-dca3'
   mv bowtie2-2.1.0-source.zip* bowtie2-2.1.0-source.zip
   unzip bowtie2-2.1.0-source
   cd bowtie2-2.1.0
   make
   cp bowtie2* /usr/local/bin

Install Samtools 
::

   cd /root
   curl -O -L http://sourceforge.net/projects/samtools/files/samtools/0.1.18/samtools-0.1.18.tar.bz2
   tar xvfj samtools-0.1.18.tar.bz2
   cd samtools-0.1.18
   make

Install `FastQC <http://www.bioinformatics.babraham.ac.uk/projects/fastqc/>`__::

   cd /usr/local/share
   curl -O http://www.bioinformatics.babraham.ac.uk/projects/fastqc/fastqc_v0.10.1.zip
   unzip fastqc_v0.10.1.zip
   chmod +x FastQC/fastqc

Install `Trimmomatic <http://www.usadellab.org/cms/?page=trimmomatic>`__ :
::

   cd /root
   curl -O http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/Trimmomatic-0.30.zip
   unzip Trimmomatic-0.30.zip
   cd Trimmomatic-0.30/
   cp trimmomatic-0.30.jar /usr/local/bin
   cp -r adapters /usr/local/share/adapters

Install `libgtextutils and fastx <http://hannonlab.cshl.edu/fastx_toolkit/>`__ :
::

   cd /root
   curl -O http://hannonlab.cshl.edu/fastx_toolkit/libgtextutils-0.6.1.tar.bz2
   tar xjf libgtextutils-0.6.1.tar.bz2 
   cd libgtextutils-0.6.1/
   ./configure && make && make install
   
   cd /root
   curl -O http://hannonlab.cshl.edu/fastx_toolkit/fastx_toolkit-0.0.13.2.tar.bz2
   tar xjf fastx_toolkit-0.0.13.2.tar.bz2
   cd fastx_toolkit-0.0.13.2/
   ./configure && make && make install

In each of these cases, we're downloading the software -- you can use
google to figure out what each package is and does if we don't discuss
it below.  We're then unpacking it, sometimes compiling it (which we
can discuss later), and then installing it for general use.

* ASK: what is libgtextutils for?

Find your data
--------------

If you downloaded the fastq files using ftp as in previous step, they should be in 'data' directory. The files usually have the '.fastq.gz' suffix.

If you see all the files you think you should, good!  Otherwise, debug.

Link your data into a working directory
---------------------------------------

Rather than *copying* the files into the working directory, let's just
*link* them in -- this creates a reference so that UNIX knows where to
find them but doesn't need to actually move them around. :
::

   cd /mnt
   mkdir -p work
   cd work
   
   ln -fs ../data/* .

(The 'ln' command does the linking, and should be provided the full path for the data directory.)

Now, do an 'ls' to list the files.  If you don't see all the files in data directory,
then the ln command above didn't work properly.  One possibility is that
your files aren't in ../data. This will link all the files in ../data (*.fastq.gz and the reference genome). 

.. note::

   This protocol takes many hours to run, so you might not want
   to run it on all the data the first time.  If you're using the
   example data, you can work with a subset of it by running this command
   INSTEAD of the `ln -fs` command above (you should be in the 'work' directory)::

      for file in ../data/*.fastq.gz
      do
          gunzip -c $file | head -400000 | gzip > $(basename $file)
      done

   This will pull out the first 100,000 reads of each file (4 lines per record)
   and put them in the current directory, which should be /mnt/work.

If you follow the above option (copying only first 100,000 reads to the 'work' directory), you should link the reference genome that is in the 'data' directory:

::

      ln -fs ../data/NC_018143.fna

For this example, lets stick to the option of using the first 100,000 reads. The 'ls' command in the current work directory should now list these three files:

NC_018143.fna  SRR671851_1.fastq.gz  SRR671851_2.fastq.gz

Evaluate the quality of your files with FastQC
----------------------------------------------

Lets use FastQC to look at the quality of your sequences::

::

      mkdir fastqc_output
      for file in ./*.fastq.gz
      do
	      fastqc $file --outdir=fastqc_output
      done

In mnt/work/fastqc_output/, you should now have the following files:

SRR671851_1_fastqc.html  SRR671851_1_fastqc.zip  SRR671851_2_fastqc.html  SRR671851_2_fastqc.zip
       
Double click on the *.html files to load them into your browser. For our example, the fastqc html reports look like this:

.. image:: fastqc_screenshot_raw_1.png
   :scale: 50 %
   :alt: SRR671851_1_fastqc
   :align: left


.. image:: fastqc_screenshot_raw_2.png
   :scale: 50 %
   :alt: SRR671851_2_fastqc
   :align: left

As you can see, the base quality (on the y-axis) decreases towards the end of the read, more so in the second read of the pair. To fix this, you can trim the read so as to exclude the poor-quality ends. Base-quality of more than 28 (the green portion in the plot) is a good threshold to decide the length of reads to keep.

Based on the quality control analysis, we will trim the reads to length 60. But first, we should first remove the Illumina adapters (short nucleotide sequences added prior to sequencing).

Find the right Illumina adapters
--------------------------------

You'll need to know which Illumina sequencing adapters were used for
your library in order to trim them off; do ::

   ls /usr/local/share/adapters/

to see which ones are available.  Below, we will use the TruSeq3-PE.fa
adapters.

.. note::

   You'll need to make sure these are the right adapters for your
   data.  If they are the right adapters, you should see that some of
   the reads are trimmed; if they're not, you won't see anything
   get trimmed.

Adapter trim each pair of files
-------------------------------

(From this point on, you may want to be running things inside of
screen, so that you detach and log out while it's running; see
:doc:`../amazon/using-screen` for more information.)

The files containing paired-end reads are labled as <prefix>_1.fastq.gz and <prefix>_2.fastq.gz. The <prefix> briefly describes the file (or is an ID, for example "SRR671851" in our case) and is exactly same for the paired read files.

For *each* of these pairs, do the following:

* ASK: where would Illumina adapters be for a person trying this for the first time.

::

   # make a temp directory
   mkdir trim
   cd trim

   # run trimmomatic
   java -jar /usr/local/bin/trimmomatic-0.30.jar PE ../SRR671851_1.fastq.gz ../SRR671851_2.fastq.gz s1_pe s1_se s2_pe s2_se ILLUMINACLIP:/usr/local/share/adapters/TruSeq3-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:20:30 AVGQUAL:28

   # compress the files and save it in the working directory
   gzip -9c s1_pe > ../SRR671851_1.pe.fq.gz
   gzip -9c s1_se > ../SRR671851_1.se.fq.gz
   gzip -9c s2_pe > ../SRR671851_2.pe.fq.gz
   gzip -9c s2_se > ../SRR671851_2.se.fq.gz

   # go back up to the working directory and remove the temp directory
   cd ..
   rm -r trim

   # make it hard to delete the files you just created
   chmod u-w *.pe.fq.gz *.se.fq.gz

To get a basic idea of what's going on, please read the '#' comments
above, but, briefly, this set of commands:

* creates a temporary directory, 'trim/'

* runs 'Trimmomatic' in that directory to trim off the adapters, and then
  puts remaining pairs (most of them!) in s1_pe and s2_pe, and any orphaned
  singletons in s1_se and s2_se. 's1' and 's2' are short for 1st and 2nd read of a pair, 'pe' stands for paired-reads, and 'se' stands for singleton reads.

* 'Trimmomatic' also does a 'sliding window trimming approach' (optional): if the quality in window of 20 drops below average score of 30, then the read is trimmed. AVGQUAL is the average quality score below which the read is dropped. LEADING and TRAILING are the number of bases to be trimmed from beginning and end of reads respectively, if the quality score drops below threshold (here, 30). Using 'Trimmomatic' to do quality control ensures that the number of paired reads in s1_pe and s2_pe are same: if any one read in a pair is dropped due to poor quality, its pair is considered as an orphan. This is important becuase several downstream tools (such as mappers) require that the number of reads in paired-end files be exactly the same. 

* Puts the trimmed reads back in the working directory

Running trimmomatic gave the following (last two lines of output):

.. note::
   Input Read Pairs: 100000 Both Surviving: 97259 (97.26%) Forward Only Surviving: 1908 (1.91%) Reverse Only Surviving: 628 (0.63%) Dropped: 205 (0.20%)
   TrimmomaticPE: Completed successfully

This means that of the 100,000 reads, 97.26% paired-reads survived the trimmomatic quality-control, 1.91% reads lost the 2nd read of the pair, 0.63% lost the 1st read of the pair, and 0.20% pairs were dropped.

At the end of this you will have new files ending in '.pe.fq.gz' and
'.se.fq.gz', representing the paired and orphaned adapter trimmed
reads, respectively.

Quality trim your reads: dropping the low-quality ends
------------------------------------------------------

If you are following this example, you should have a bunch of '.pe.fq.gz' files and
a bunch of '.se.fq.gz' files, and html FastQC reports to determine the quality of data in these files. If your individual fastq files need trimming, use the following syntax:

	gunzip -c <'.fq.gz' file> | fastx_trimmer -l <length to keep> -z -o <'.qc.fq.gz' outfile>

This first uncompresses the .fq.gz files, trims the reads to specified length, and -z compresses output with gzip.

.. note::
	If you have several files to run FastQC on, you can unzip the '_fastqc.zip' files, and look for 'summary.txt' file in the unzipped folders. If you see 'FAIL' or 'WARN' next to 'Per base sequence quality' in this 'summary.txt', it is a good idea to look at the '_fastqc.html' files individually and trim the ends, or decide if the data quality isn't good enough for downstream analyses, and if not, you may not want to use that file. 
	
	A simple script can be written to identify the .fq.gz files that FAIL the 'Per base sequence quality' test, and will narrow down the number of FastQC html reports to look at.

Automating this step
~~~~~~~~~~~~~~~~~~~~

If all .fq.gz files need trimming (as we will in this example), do the following: 

.. ::

   echo 1-quality FILTER `date` >> /root/times.out

This step can be automated with a 'for' loop at the shell prompt.  Try :
::

   for file in *.pe.fq.gz *.se.fq.gz
   do
        echo working with $file
        newfile="$(basename $file .fq.gz)"
        gunzip -c $file | fastx_trimmer -l 60 | gzip -9c > "${newfile}.qc.fq.gz"
   done

What this loop does is:

* for every file ending in pe.fq.gz and se.fq.gz,

* print out a message with the filename,

* uncompresses the original file, passes it through fastx_trimmer, recompresses it,
  and saves it as 'newfile'.qc.fq.gz

Re-evaluate the quality of your files with FastQC
-------------------------------------------------

Since we did quality-control using Trimmomatic and dropped low-quality ends of reads, lets re-run fastqc to confirm that the data is now good enough for downstream analysis.

::

	  for file in *.qc.fq.gz
      do
	      fastqc $file --outdir=fastqc_output
      done

This will generate *.qc.fq_fastqc.html files in the fastqc_output directory.

SRR671851_1.pe.qc.fq_fastqc.html:

.. image:: fastqc_qc_1_pe.png
   :scale: 50 %
   :alt: fastqc_qc_1_pe
   :align: left

SRR671851_1.se.qc.fq_fastqc.html

.. image:: fastqc_qc_1_se.png
   :scale: 50 %
   :alt: fastqc_qc_1_se.png
   :align: left

SRR671851_2.pe.qc.fq_fastqc.html

.. image:: fastqc_qc_2_pe.png
   :scale: 50 %
   :alt: fastqc_qc_2_pe
   :align: left

SRR671851_2.se.qc.fq_fastqc.html

.. image:: fastqc_qc_2_se.png
   :scale: 50 %
   :alt: fastqc_qc_2_se.png
   :align: left

The data now looks good (base-quality scores for all four files lie in the green region)! 

Finishing up
------------

You should now have a bunch of files in the working directory:

   *_1.fastq.gz 	      	 		     - the original data
   *_2.fastq.gz
   *_1.pe.fq.gz, *_2.pe.fq.gz		     - adapter trimmed and filtered pe
   *_1.se.fq.gz, *_2.se.fq.gz		     - adapter trimmed and filtered se
   *.qc.fq.gz							 - FASTX trimmed files

Yikes!  What to do?

Well, first, you can get rid of the original data.  You already have it on a
disk somewhere, right? :
::

   rm *.fastq.gz

Next, you can get rid of the 'pe.fq.gz' and 'se.fq.gz' files, since you
only want the QC files.  So :
::

   rm *.pe.fq.gz *.se.fq.gz

Things to think about
~~~~~~~~~~~~~~~~~~~~~

Note that the filenames, while ugly, are conveniently structured with the
history of what you've done.  This is a good idea.

Also note that we've conveniently named the files so that we can remove
the unwanted ones en masse.  This is a good idea, too.

And finally, make the end product files read-only :
::

   chmod u-w *.qc.fq.gz

to make sure you don't accidentally delete something.

Saving the files
----------------

At this point, you should save these files::

   mkdir save
   cp *.qc.fq.gz save
   du -sk save

If you are running with a data subset (as in this example), do
::

   cp /mnt/work/*.qc.fq.gz ../data

to save the QC files for later use.

.. shell stop

This puts the data you want to save into a subdirectory named 'save', and
calculates the size.

Now, create a volume of the given size -- divide by a thousand to get
gigabytes, multiply by 1.1 to make sure you have enough room, and then
follow the instructions in :doc:`../amazon/index`.  Once
you've mounted it properly (I would suggest mounting it on /save
instead of /data!), then do ::

   rsync -av save /save

which will copy all of the files over from the ./save directory onto the
'/save' disk.  Then 'umount /save' and voila, you've got a copy of the files!

Next stop: :doc:`2-mapping`.

